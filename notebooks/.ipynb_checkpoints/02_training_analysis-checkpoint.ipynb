{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970c9a96",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Setup and Imports\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Image, display\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 5)\n",
    "\n",
    "results_dir = Path(\"../results\")\n",
    "logs_dir = Path(\"../models/saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef27b64",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Load Training Logs\n",
    "log_files = sorted(logs_dir.glob(\"training_log_*.json\"))\n",
    "\n",
    "all_logs = []\n",
    "for file in log_files:\n",
    "    with open(file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    model_name = \"vit\" if \"vit\" in file.name.lower() else \"resnet\"\n",
    "    df = pd.DataFrame(data)\n",
    "    df[\"model\"] = model_name\n",
    "    all_logs.append(df)\n",
    "\n",
    "train_df = pd.concat(all_logs, ignore_index=True) if all_logs else pd.DataFrame()\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911bfcc5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 3. Accuracy/Loss over Epochs\n",
    "if not train_df.empty:\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    sns.lineplot(data=train_df, x=\"epoch\", y=\"train_acc\", hue=\"model\", marker=\"o\", ax=ax[0])\n",
    "    ax[0].set_title(\"Training Accuracy over Epochs\"); ax[0].set_ylim(0, 1)\n",
    "    sns.lineplot(data=train_df, x=\"epoch\", y=\"val_acc\", hue=\"model\", marker=\"o\", ax=ax[1])\n",
    "    ax[1].set_title(\"Validation Accuracy over Epochs\"); ax[1].set_ylim(0, 1)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    plt.figure(figsize=(7,5))\n",
    "    sns.lineplot(data=train_df, x=\"epoch\", y=\"val_loss\", hue=\"model\", marker=\"o\")\n",
    "    plt.title(\"Validation Loss over Epochs\"); plt.tight_layout(); plt.show()\n",
    "else:\n",
    "    print(\"No training logs found in models/saved/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa6acb0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 4. Load and Compare Final Evaluation Metrics\n",
    "metric_files = sorted(results_dir.glob(\"metrics_summary_*.csv\"))\n",
    "eval_data = []\n",
    "for file in metric_files:\n",
    "    df = pd.read_csv(file)\n",
    "    df[\"model\"] = file.stem.replace(\"metrics_summary_\", \"\")\n",
    "    eval_data.append(df)\n",
    "\n",
    "eval_df = pd.concat(eval_data, ignore_index=True) if eval_data else pd.DataFrame()\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35874a4c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 5. Evaluation Metrics Comparison\n",
    "if not eval_df.empty:\n",
    "    melted = eval_df.melt(id_vars=[\"model\"], var_name=\"metric\", value_name=\"score\")\n",
    "    plt.figure(figsize=(8,5))\n",
    "    sns.barplot(data=melted, x=\"metric\", y=\"score\", hue=\"model\", palette=\"Set2\")\n",
    "    plt.title(\"Evaluation Metrics Comparison (ResNet vs ViT)\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.tight_layout(); plt.show()\n",
    "else:\n",
    "    print(\"No evaluation metrics found in results/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21514f01",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 6. Cross-Generator Generalization Analysis\n",
    "cross_files = sorted(results_dir.glob(\"cross_generator_results_*.csv\"))\n",
    "cross_dfs = []\n",
    "for file in cross_files:\n",
    "    df = pd.read_csv(file)\n",
    "    df[\"model\"] = file.stem.replace(\"cross_generator_results_\", \"\")\n",
    "    cross_dfs.append(df)\n",
    "\n",
    "cross_df = pd.concat(cross_dfs, ignore_index=True) if cross_dfs else pd.DataFrame()\n",
    "cross_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf692d0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 7. Cross-Generator Bars (Accuracy/F1)\n",
    "if not cross_df.empty:\n",
    "    melted = cross_df.melt(id_vars=[\"model\",\"test_generator\"], value_vars=[\"accuracy\",\"f1\"], var_name=\"metric\", value_name=\"value\")\n",
    "    plt.figure(figsize=(12,5))\n",
    "    sns.barplot(data=melted, x=\"test_generator\", y=\"value\", hue=\"model\")\n",
    "    plt.title(\"Cross-Generator Generalization (Accuracy/F1 per Generator)\")\n",
    "    plt.ylabel(\"Score\"); plt.ylim(0, 1); plt.xticks(rotation=30, ha=\"right\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "else:\n",
    "    print(\"No cross-generator results found in results/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a069fc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 8. Confusion Matrices\n",
    "cm_files = sorted(results_dir.glob(\"confusion_matrix_*.png\"))\n",
    "for cm_path in cm_files:\n",
    "    print(cm_path.name)\n",
    "    display(Image(filename=str(cm_path)))\n",
    "if not cm_files:\n",
    "    print(\"No confusion matrix images found in results/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809b7fe0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 9. Summary Table\n",
    "if not eval_df.empty:\n",
    "    summary = eval_df.groupby(\"model\")[[\"accuracy\",\"precision\",\"recall\",\"f1\"]].mean().sort_values(\"f1\", ascending=False)\n",
    "    display(summary)\n",
    "else:\n",
    "    print(\"No evaluation metrics to summarize.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e14ccce",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 10. Export Summary\n",
    "if not eval_df.empty:\n",
    "    out_dir = results_dir / \"analysis_exports\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    summary = eval_df.groupby(\"model\")[[\"accuracy\",\"precision\",\"recall\",\"f1\"]].mean()\n",
    "    summary.to_csv(out_dir / \"final_metrics_summary.csv\")\n",
    "    print(f\"Exported: {out_dir / 'final_metrics_summary.csv'}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
